{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zsBuRChI-1om",
        "outputId": "77c038b9-64f4-4a74-c75b-1d625cedd468"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hmmlearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhmmlearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianHMM\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hmmlearn'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
        "\n",
        "# === Feature Set Used ===\n",
        "features = [\n",
        "    \"gn_distribution_balance_wbtc\",\n",
        "    \"gn_addresses_supply_balance_more_100k\",\n",
        "    \"gn_derivatives_futures_open_interest_crypto_margin_sum\",\n",
        "    \"gn_supply_active_3y_5y\",\n",
        "    \"gn_market_realized_volatility_3_months\",\n",
        "    \"cq_reserve_usd\",\n",
        "    \"gn_supply_active_more_1y_percent\",\n",
        "    \"gn_supply_illiquid_sum\",\n",
        "    \"gn_blockchain_utxo_loss_count\",\n",
        "    \"gn_derivatives_futures_annualized_basis_3m\"\n",
        "]\n",
        "# === Load and Prepare Data ===\n",
        "print(\"Loading and preparing data...\")\n",
        "df = pd.read_csv(\"btc_features_output.csv\", parse_dates=[\"start_time\"])\n",
        "df['start_time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
        "df = df.sort_values(\"start_time\").reset_index(drop=True)\n",
        "df.rename(columns={'unified_close': 'price'}, inplace=True)\n",
        "\n",
        "# Check for missing values and handle them\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "missing_pct = df[features + [\"price\"]].isna().mean() * 100\n",
        "print(f\"Missing values percentage:\\n{missing_pct}\")\n",
        "\n",
        "# Drop rows with missing values in the critical features\n",
        "df = df.dropna(subset=features + [\"price\"])\n",
        "print(f\"Clean data shape after removing NAs: {df.shape}\")\n",
        "\n",
        "# === Feature Engineering ===\n",
        "# Add price momentum features\n",
        "df['price_1d_change'] = df['price'].pct_change(1)\n",
        "df['price_7d_change'] = df['price'].pct_change(7)\n",
        "df['price_volatility'] = df['price'].pct_change().rolling(7).std().fillna(0)\n",
        "\n",
        "# Add volume-related features\n",
        "if 'volume' in df.columns:\n",
        "    df['volume_change'] = df['volume'].pct_change().fillna(0)\n",
        "    df['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(7).mean().fillna(df['volume'])\n",
        "    features.extend(['volume_change', 'volume_ma_ratio'])\n",
        "\n",
        "# Add price-related features to the feature set\n",
        "features.extend(['price_1d_change', 'price_7d_change', 'price_volatility'])\n",
        "\n",
        "# Remove rows after feature engineering that might have NaNs\n",
        "df = df.dropna(subset=features).reset_index(drop=True)\n",
        "\n",
        "# === Feature Selection and Scaling ===\n",
        "print(\"Scaling features...\")\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(df[features])\n",
        "\n",
        "# === Train-Test Split ===\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "df_train = df.iloc[:train_size].copy()\n",
        "df_test = df.iloc[train_size:].copy()\n",
        "\n",
        "# === BIC Score for Model Selection ===\n",
        "def compute_bic_score(X, model):\n",
        "    \"\"\"Compute the BIC score for the fitted model.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    n_components = model.n_components\n",
        "    n_parameters = n_components * n_features + n_components * (n_components - 1)  # mean + transition matrix\n",
        "\n",
        "    log_likelihood = model.score(X)\n",
        "    bic = -2 * log_likelihood + n_parameters * np.log(n_samples)\n",
        "    return bic\n",
        "\n",
        "# === Find Optimal Number of Components ===\n",
        "print(\"Finding optimal number of components...\")\n",
        "n_components_range = range(2, 10)\n",
        "models = []\n",
        "bic_scores = []\n",
        "aic_scores = []\n",
        "\n",
        "for n_components in n_components_range:\n",
        "    model = GaussianHMM(\n",
        "        n_components=n_components,\n",
        "        covariance_type=\"diag\",\n",
        "        n_iter=1000,\n",
        "        tol=1e-4,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train)\n",
        "    models.append(model)\n",
        "\n",
        "    # Compute BIC score\n",
        "    bic = compute_bic_score(X_train, model)\n",
        "    bic_scores.append(bic)\n",
        "\n",
        "    # Compute AIC score0\n",
        "    log_likelihood = model.score(X_train)\n",
        "    n_features = X_train.shape[1]\n",
        "    n_params = n_components * (n_features + n_features + n_components - 1)\n",
        "    aic = -2 * log_likelihood + 2 * n_params\n",
        "    aic_scores.append(aic)\n",
        "\n",
        "    print(f\"Components: {n_components}, BIC: {bic:.2f}, AIC: {aic:.2f}\")\n",
        "\n",
        "# Plot BIC and AIC scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(n_components_range, bic_scores, '-o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('BIC Score')\n",
        "plt.title('BIC Score vs Number of Components')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(n_components_range, aic_scores, '-o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('AIC Score')\n",
        "plt.title('AIC Score vs Number of Components')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"model_selection_scores.png\")\n",
        "\n",
        "# Select optimal number of components based on BIC score (lower is better)\n",
        "optimal_components = n_components_range[np.argmin(bic_scores)]\n",
        "print(f\"Optimal number of components based on BIC: {optimal_components}\")\n",
        "\n",
        "# === Train Gaussian HMM with Optimal Components (5 for this enhanced version) ===\n",
        "print(\"Training HMM model with 5 components...\")\n",
        "model = GaussianHMM(\n",
        "    n_components=5,  # Using 5 regimes for enhanced version\n",
        "    covariance_type=\"diag\",\n",
        "    n_iter=2000,\n",
        "    tol=1e-5,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'Enhanced_HMM_Model.pkl')\n",
        "joblib.dump(scaler, 'Feature_Scaler.pkl')\n",
        "\n",
        "# === Regime Assignment ===\n",
        "print(\"Assigning regimes...\")\n",
        "df_train[\"Regime\"] = model.predict(X_train)\n",
        "df_train[\"Return\"] = df_train[\"price\"].pct_change().fillna(0)\n",
        "\n",
        "# Analyze regime characteristics for labeling\n",
        "regime_stats = pd.DataFrame()\n",
        "for regime in range(5):\n",
        "    regime_data = df_train[df_train[\"Regime\"] == regime]\n",
        "    stats = {\n",
        "        'avg_return': regime_data[\"Return\"].mean() * 100,\n",
        "        'volatility': regime_data[\"Return\"].std() * 100,\n",
        "        'median_return': regime_data[\"Return\"].median() * 100,\n",
        "        'count': len(regime_data),\n",
        "        'pct_positive_days': (regime_data[\"Return\"] > 0).mean() * 100\n",
        "    }\n",
        "    regime_stats = pd.concat([regime_stats, pd.DataFrame([stats])], ignore_index=True)\n",
        "\n",
        "# Label regimes based on return and volatility characteristics\n",
        "regime_stats.index = range(5)\n",
        "print(\"\\n=== Regime Statistics ===\")\n",
        "print(regime_stats)\n",
        "\n",
        "# Sort regimes by average return for labeling\n",
        "sorted_regimes = regime_stats.sort_values('avg_return')\n",
        "regime_map = {}\n",
        "\n",
        "# Assign labels to regimes\n",
        "labels = [\"Strong Bear\", \"Weak Bear\", \"Neutral\", \"Weak Bull\", \"Strong Bull\"]\n",
        "for i, idx in enumerate(sorted_regimes.index):\n",
        "    regime_map[int(idx)] = labels[i]\n",
        "\n",
        "print(\"\\n=== Regime Labels ===\")\n",
        "for regime, label in regime_map.items():\n",
        "    print(f\"Regime {regime}: {label} (Avg Return: {regime_stats.loc[regime, 'avg_return']:.2f}%, Volatility: {regime_stats.loc[regime, 'volatility']:.2f}%)\")\n",
        "\n",
        "# === Predict Test Set Regimes ===\n",
        "print(\"Predicting test set regimes...\")\n",
        "test_predictions = []\n",
        "window_size = 30\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    sequence = np.vstack((X_train, X_test[:i+1]))[-window_size:]\n",
        "    current_state = model.predict(sequence)[-1]\n",
        "    test_predictions.append(current_state)\n",
        "\n",
        "df_test[\"Regime\"] = test_predictions\n",
        "df_test[\"Market_Regime\"] = df_test[\"Regime\"].map(regime_map)\n",
        "\n",
        "# === Combine Train & Test Sets ===\n",
        "df_all = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
        "df_all[\"Market_Regime\"] = df_all[\"Regime\"].map(regime_map)\n",
        "\n",
        "# === Regime Transition Analysis ===\n",
        "print(\"\\n=== Regime Transition Matrix ===\")\n",
        "print(pd.DataFrame(model.transmat_,\n",
        "                   index=[regime_map[i] for i in range(5)],\n",
        "                   columns=[regime_map[i] for i in range(5)]))\n",
        "\n",
        "# === Feature Analysis ===\n",
        "print(\"\\n=== Feature Importance in Each Regime ===\")\n",
        "feature_importance = pd.DataFrame()\n",
        "\n",
        "for i in range(5):\n",
        "    means = model.means_[i]\n",
        "    stds = np.sqrt(model.covars_[i])\n",
        "\n",
        "    # Z-scores of feature means relative to overall distribution\n",
        "    importance = pd.Series(means, index=features)\n",
        "    feature_importance[regime_map[i]] = importance\n",
        "\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualize feature importance by regime\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(feature_importance, annot=True, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Feature Importance by Market Regime\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_importance.png\")\n",
        "\n",
        "# === Regime Duration Analysis ===\n",
        "df_all['regime_change'] = df_all['Regime'].ne(df_all['Regime'].shift()).astype(int)\n",
        "df_all['regime_group'] = df_all['regime_change'].cumsum()\n",
        "\n",
        "regime_durations = df_all.groupby(['regime_group', 'Market_Regime']).size().reset_index(name='duration')\n",
        "print(\"\\n=== Regime Duration Analysis ===\")\n",
        "duration_stats = regime_durations.groupby('Market_Regime')['duration'].agg(['mean', 'median', 'min', 'max'])\n",
        "print(duration_stats)\n",
        "\n",
        "# Plot regime duration distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Market_Regime', y='duration', data=regime_durations)\n",
        "plt.title('Distribution of Regime Durations')\n",
        "plt.ylabel('Duration (days)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"regime_durations.png\")\n",
        "\n",
        "# === Visualization of Market Regimes ===\n",
        "plt.figure(figsize=(14, 8))\n",
        "colors = {\"Strong Bull\": \"darkgreen\", \"Weak Bull\": \"lightgreen\",\n",
        "          \"Neutral\": \"yellow\", \"Weak Bear\": \"salmon\", \"Strong Bear\": \"darkred\"}\n",
        "\n",
        "# Create background for regimes\n",
        "for regime, color in colors.items():\n",
        "    mask = df_all[\"Market_Regime\"] == regime\n",
        "    if mask.any():\n",
        "        plt.scatter(df_all.loc[mask, \"start_time\"], df_all.loc[mask, \"price\"],\n",
        "                    c=color, label=regime, alpha=0.7, s=10)\n",
        "\n",
        "plt.title(\"Cryptocurrency Price with Market Regimes\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./results/market_regimes_visualization.png\")\n",
        "\n",
        "# === Confusion Matrix for Regimes ===\n",
        "# Compute next day regime predictions based on current day\n",
        "df_all['pred_next_regime'] = df_all['Regime'].shift(1)\n",
        "df_all['actual_regime'] = df_all['Regime']\n",
        "\n",
        "# Create confusion matrix\n",
        "confusion = pd.crosstab(df_all['actual_regime'].dropna(),\n",
        "                         df_all['pred_next_regime'].dropna(),\n",
        "                         rownames=['Actual'],\n",
        "                         colnames=['Predicted'])\n",
        "\n",
        "# Convert to actual regime names\n",
        "confusion.index = [regime_map[i] for i in confusion.index]\n",
        "confusion.columns = [regime_map[i] for i in confusion.columns]\n",
        "\n",
        "print(\"\\n=== Regime Prediction Confusion Matrix ===\")\n",
        "print(confusion)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Regime Prediction Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"regime_confusion_matrix.png\")\n",
        "\n",
        "# === Regime Prediction Function ===\n",
        "def predict_regime(features_df, window_size=30):\n",
        "    \"\"\"\n",
        "    Predict the current market regime based on recent feature data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    features_df : DataFrame with the same features used for training\n",
        "    window_size : How many days of data to use for prediction\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    regime : The predicted market regime\n",
        "    probabilities : Probability distribution over all regimes\n",
        "    \"\"\"\n",
        "    # Ensure we have the right features\n",
        "    missing_features = set(features) - set(features_df.columns)\n",
        "    if missing_features:\n",
        "        raise ValueError(f\"Missing features: {missing_features}\")\n",
        "\n",
        "    # Scale the features\n",
        "    X = scaler.transform(features_df[features].tail(window_size))\n",
        "\n",
        "    # Predict hidden states\n",
        "    states = model.predict(X)\n",
        "    current_state = states[-1]\n",
        "\n",
        "    # Get transition probabilities from current state\n",
        "    next_state_probs = model.transmat_[current_state]\n",
        "\n",
        "    return regime_map[current_state], {regime_map[i]: prob for i, prob in enumerate(next_state_probs)}\n",
        "\n",
        "# === Model Performance Metrics ===\n",
        "print(\"\\n=== Model Performance Analysis ===\")\n",
        "\n",
        "# 1. Predictive Accuracy\n",
        "df_all['correct_prediction'] = (df_all['pred_next_regime'] == df_all['actual_regime'])\n",
        "accuracy = df_all['correct_prediction'].mean()\n",
        "print(f\"Next-Day Regime Prediction Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# 2. Feature Importance Analysis\n",
        "def feature_importance_analysis(model, feature_names):\n",
        "    \"\"\"Analyze which features are most important for distinguishing between regimes\"\"\"\n",
        "    feature_importance = np.zeros(len(feature_names))\n",
        "\n",
        "    for i in range(model.n_components):\n",
        "        for j in range(model.n_components):\n",
        "            if i != j:\n",
        "                # Calculate the difference in means between regimes\n",
        "                diff = np.abs(model.means_[i] - model.means_[j])\n",
        "                feature_importance += diff\n",
        "\n",
        "    # Normalize\n",
        "    feature_importance = feature_importance / feature_importance.sum()\n",
        "    return pd.Series(feature_importance, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "importance = feature_importance_analysis(model, features)\n",
        "print(\"\\nFeature Importance for Regime Classification:\")\n",
        "print(importance)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "importance.plot(kind='bar')\n",
        "plt.title('Feature Importance for Regime Classification')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_importance_bar.png\")\n",
        "\n",
        "# 3. Model Stability Analysis\n",
        "print(\"\\n=== Model Stability Analysis ===\")\n",
        "# Check regime distribution over time\n",
        "regime_counts = df_all.groupby(pd.Grouper(key='start_time', freq='M'))['Market_Regime'].value_counts().unstack().fillna(0)\n",
        "regime_counts = regime_counts.div(regime_counts.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "regime_counts.plot(kind='area', stacked=True, colormap='viridis')\n",
        "plt.title('Regime Distribution Over Time')\n",
        "plt.ylabel('Proportion')\n",
        "plt.legend(title='Market Regime')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"regime_distribution_time.png\")\n",
        "\n",
        "# === Feature Correlation within Regimes ===\n",
        "# Analyze how features correlate differently in different regimes\n",
        "plt.figure(figsize=(20, 16))\n",
        "for i, regime in enumerate(regime_map.values()):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    regime_data = df_all[df_all['Market_Regime'] == regime]\n",
        "\n",
        "    if len(regime_data) > 10:  # Need enough data for correlation\n",
        "        corr_matrix = regime_data[features].corr()\n",
        "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(f'Feature Correlation in {regime} Regime')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f\"Insufficient data for {regime} regime\",\n",
        "                 horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_correlations_by_regime.png\")\n",
        "\n",
        "# === Price Change Distribution by Regime ===\n",
        "plt.figure(figsize=(12, 8))\n",
        "for regime in regime_map.values():\n",
        "    regime_returns = df_all[df_all['Market_Regime'] == regime]['Return'].dropna()\n",
        "    if len(regime_returns) > 10:\n",
        "        sns.kdeplot(regime_returns, label=regime)\n",
        "\n",
        "plt.title('Return Distribution by Market Regime')\n",
        "plt.xlabel('Daily Return')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"return_distribution_by_regime.png\")\n",
        "\n",
        "# === Save Labeled Data ===\n",
        "output_cols = [\"start_time\", \"price\", \"Regime\", \"Market_Regime\", \"Return\"] + features\n",
        "df_all[output_cols].to_csv(\"hmm_labeled_data.csv\", index=False)\n",
        "\n",
        "print(\"\\n=== Final Regime Analysis ===\")\n",
        "print(f\"Current Market Regime: {df_all['Market_Regime'].iloc[-1]}\")\n",
        "\n",
        "# Predict next regime\n",
        "current_features = df_all[features].tail(30)\n",
        "next_regime, probabilities = predict_regime(df_all[features], window_size=30)\n",
        "\n",
        "print(f\"Predicted Next Regime: {next_regime}\")\n",
        "print(\"Transition Probabilities:\")\n",
        "for regime, prob in probabilities.items():\n",
        "    print(f\"  {regime}: {prob:.2%}\")\n",
        "\n",
        "# Final regime summary\n",
        "regime_summary = df_all.groupby('Market_Regime').agg({\n",
        "    'Return': ['mean', 'std', 'median'],\n",
        "    'price': ['count', 'min', 'max']\n",
        "}).round(4)\n",
        "\n",
        "print(\"\\n=== Regime Summary Statistics ===\")\n",
        "print(regime_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmAxk6roEiS"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubunWmsOG2Dg",
        "outputId": "ef6c4bf4-b4a6-4666-c6b0-2779ca968825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data shape: (8343, 61)\n",
            "Missing values percentage:\n",
            "gn_distribution_balance_wbtc                              0.023972\n",
            "gn_addresses_supply_balance_more_100k                     0.011986\n",
            "gn_derivatives_futures_open_interest_crypto_margin_sum    0.011986\n",
            "gn_supply_active_3y_5y                                    0.023972\n",
            "gn_market_realized_volatility_3_months                    0.023972\n",
            "cq_reserve_usd                                            0.000000\n",
            "gn_supply_active_more_1y_percent                          0.023972\n",
            "gn_supply_illiquid_sum                                    0.023972\n",
            "gn_blockchain_utxo_loss_count                             0.011986\n",
            "gn_derivatives_futures_annualized_basis_3m                0.011986\n",
            "price                                                     0.023972\n",
            "dtype: float64\n",
            "\n",
            "Number of data points in each regime:\n",
            "predicted_regime\n",
            "0    2647\n",
            "1    2913\n",
            "2    1417\n",
            "3    1365\n",
            "4       1\n",
            "Name: count, dtype: int64\n",
            "              start_time  predicted_regime\n",
            "8333 2024-04-15 01:00:00                 3\n",
            "8334 2024-04-15 03:00:00                 3\n",
            "8335 2024-04-15 04:00:00                 3\n",
            "8336 2024-04-15 06:00:00                 3\n",
            "8337 2024-04-15 09:00:00                 3\n",
            "8338 2024-04-15 10:00:00                 3\n",
            "8339 2024-04-15 11:00:00                 3\n",
            "8340 2024-04-15 12:00:00                 3\n",
            "8341 2024-04-15 17:00:00                 3\n",
            "8342 2024-04-15 23:00:00                 3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib  # Ensure joblib is imported\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === Load the trained model and scaler ===\n",
        "with open(\"Enhanced_HMM_Model.pkl\", \"rb\") as f:\n",
        "    hmm_model = joblib.load(f)\n",
        "\n",
        "with open(\"Feature_Scaler.pkl\", \"rb\") as f:\n",
        "    scaler = joblib.load(f)\n",
        "\n",
        "# === Load new test dataset ===\n",
        "data_path = \"btc_features_output.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# === Define the same features used during training ===\n",
        "features = [\n",
        "    \"gn_distribution_balance_wbtc\",\n",
        "    \"gn_addresses_supply_balance_more_100k\",\n",
        "    \"gn_derivatives_futures_open_interest_crypto_margin_sum\",\n",
        "    \"gn_supply_active_3y_5y\",\n",
        "    \"gn_market_realized_volatility_3_months\",\n",
        "    \"cq_reserve_usd\",\n",
        "    \"gn_supply_active_more_1y_percent\",\n",
        "    \"gn_supply_illiquid_sum\",\n",
        "    \"gn_blockchain_utxo_loss_count\",\n",
        "    \"gn_derivatives_futures_annualized_basis_3m\"\n",
        "]\n",
        "\n",
        "df.rename(columns={'unified_close': 'price'}, inplace=True)\n",
        "df['start_time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
        "df = df.sort_values(\"start_time\").reset_index(drop=True)\n",
        "\n",
        "# Check for missing values and handle them\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "missing_pct = df[features + [\"price\"]].isna().mean() * 100\n",
        "print(f\"Missing values percentage:\\n{missing_pct}\")\n",
        "\n",
        "# === Feature Engineering ===\n",
        "df['price_1d_change'] = df['price'].pct_change(1)\n",
        "df['price_7d_change'] = df['price'].pct_change(7)\n",
        "df['price_volatility'] = df['price'].pct_change().rolling(7).std().fillna(0)\n",
        "\n",
        "if 'volume' in df.columns:\n",
        "    df['volume_change'] = df['volume'].pct_change().fillna(0)\n",
        "    df['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(7).mean().fillna(df['volume'])\n",
        "    features.extend(['volume_change', 'volume_ma_ratio'])\n",
        "\n",
        "features.extend(['price_1d_change', 'price_7d_change', 'price_volatility'])\n",
        "\n",
        "# === Extract these features from the test data ===\n",
        "X_test = df[features]\n",
        "\n",
        "# Handle missing values in feature columns (mean imputation)\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "\n",
        "# === Apply the same scaler to the test features ===\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# === Use the model to predict regimes ===\n",
        "predicted_regimes = hmm_model.predict(X_test_scaled)\n",
        "\n",
        "# === Attach the prediction result to the original dataframe ===\n",
        "df[\"predicted_regime\"] = predicted_regimes\n",
        "\n",
        "# === Count occurrences of each regime ===\n",
        "regime_counts = df[\"predicted_regime\"].value_counts().sort_index()\n",
        "print(\"\\nNumber of data points in each regime:\")\n",
        "print(regime_counts)\n",
        "\n",
        "# === Optionally save it to a new CSV ===\n",
        "df.to_csv(\"btc_test_data_with_predictions.csv\", index=False)\n",
        "\n",
        "# === Preview last few predictions ===\n",
        "print(df[[\"start_time\", \"predicted_regime\"]].tail(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXllPfvNnq5n"
      },
      "source": [
        "# Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTvuzPzTayNs",
        "outputId": "6c32fd05-6c6d-4793-f56c-4745852619ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Strategy Performance Summary ===\n",
            "Total PnL: -2.8826\n",
            "Number of Trades: 40\n",
            "Trading Period: 19529 days\n",
            "Trade Frequency: 0.00 trades/day\n",
            "Win Rate: 50.00%\n",
            "Average Trade Return: -0.0721\n",
            "Max Drawdown: -2.8826\n",
            "Sharpe Ratio: -0.3202\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === Load your prediction dataset ===\n",
        "df = pd.read_csv(\"btc_test_data_with_predictions.csv\")\n",
        "df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
        "\n",
        "# === Compute price metrics ===\n",
        "df['price_1d_change'] = df['price'].pct_change()\n",
        "df['price_volatility'] = df['price'].pct_change().rolling(7).std()\n",
        "df['price_mean'] = df['price'].rolling(20).mean()\n",
        "df['price_std'] = df['price'].rolling(20).std()\n",
        "df['price_zscore'] = (df['price'] - df['price_mean']) / df['price_std']\n",
        "\n",
        "# === Define parameters ===\n",
        "fee_rate = 0.0006  # 0.06%\n",
        "df['position'] = 0\n",
        "df['trade'] = 0\n",
        "df['pnl'] = 0.0\n",
        "df['cum_pnl'] = 0.0\n",
        "df['trade_return'] = 0.0\n",
        "\n",
        "# === Initialize position, PnL tracker ===\n",
        "position = 0\n",
        "cash_pnl = 0.0\n",
        "trade_returns = []\n",
        "\n",
        "# === Strategy logic ===\n",
        "for i in range(1, len(df)):\n",
        "    regime = df.loc[i, 'predicted_regime']\n",
        "    momentum = df.loc[i, 'price_1d_change']\n",
        "    vol = df.loc[i, 'price_volatility']\n",
        "    zscore = df.loc[i, 'price_zscore']\n",
        "    price_now = df.loc[i, 'price']\n",
        "\n",
        "    new_position = position\n",
        "\n",
        "    if regime == 4:  # Bull\n",
        "        if momentum > 0.005 and 0.01 < vol < 0.03 and zscore < 2:\n",
        "            new_position = 1\n",
        "    elif regime == 0:  # Bear\n",
        "        if momentum < -0.005 and vol > 0.03 and zscore > -2:\n",
        "            new_position = -1\n",
        "    else:\n",
        "        new_position = 0\n",
        "\n",
        "    if new_position != position:\n",
        "        trade_value = price_now * abs(new_position - position)\n",
        "\n",
        "        trade_pnl = 0\n",
        "\n",
        "        if position == 1:\n",
        "            trade_pnl += price_now * 1\n",
        "            trade_pnl -= fee_rate * price_now * 1\n",
        "        elif position == -1:\n",
        "            trade_pnl -= price_now * 1\n",
        "            trade_pnl -= fee_rate * price_now * 1\n",
        "\n",
        "        if new_position == 1:\n",
        "            trade_pnl -= price_now * 1\n",
        "            trade_pnl -= fee_rate * price_now * 1\n",
        "        elif new_position == -1:\n",
        "            trade_pnl += price_now * 1\n",
        "            trade_pnl -= fee_rate * price_now * 1\n",
        "\n",
        "        cash_pnl += trade_pnl\n",
        "        df.loc[i, 'trade'] = 1\n",
        "        df.loc[i, 'trade_return'] = trade_pnl\n",
        "        if trade_pnl != 0:\n",
        "            trade_returns.append(trade_pnl)\n",
        "\n",
        "        position = new_position\n",
        "\n",
        "    df.loc[i, 'position'] = position\n",
        "    df.loc[i, 'pnl'] = cash_pnl\n",
        "    df.loc[i, 'cum_pnl'] = cash_pnl\n",
        "\n",
        "# === Summary Stats ===\n",
        "total_pnl = df['cum_pnl'].iloc[-1]\n",
        "num_trades = df['trade'].sum()\n",
        "\n",
        "# Trading period\n",
        "period_days = (df['start_time'].max() - df['start_time'].min()).days\n",
        "trades_per_day = num_trades / period_days if period_days else 0\n",
        "\n",
        "# Win rate\n",
        "wins = len([x for x in trade_returns if x > 0])\n",
        "win_rate = wins / len(trade_returns) if trade_returns else 0\n",
        "\n",
        "# Average return per trade\n",
        "avg_trade_return = np.mean(trade_returns) if trade_returns else 0\n",
        "\n",
        "# Max Drawdown\n",
        "df['peak'] = df['cum_pnl'].cummax()\n",
        "df['drawdown'] = df['cum_pnl'] - df['peak']\n",
        "max_drawdown = df['drawdown'].min()\n",
        "\n",
        "# Sharpe Ratio (daily returns, risk-free rate 0)\n",
        "daily_pnl = df['cum_pnl'].diff()\n",
        "sharpe_ratio = (daily_pnl.mean() / daily_pnl.std()) * np.sqrt(365) if daily_pnl.std() != 0 else 0\n",
        "\n",
        "# === Print Summary ===\n",
        "print(\"=== Strategy Performance Summary ===\")\n",
        "print(f\"Total PnL: {total_pnl:.4f}\")\n",
        "print(f\"Number of Trades: {int(num_trades)}\")\n",
        "print(f\"Trading Period: {period_days} days\")\n",
        "print(f\"Trade Frequency: {trades_per_day:.2f} trades/day\")\n",
        "print(f\"Win Rate: {win_rate*100:.2f}%\")\n",
        "print(f\"Average Trade Return: {avg_trade_return:.4f}\")\n",
        "print(f\"Max Drawdown: {max_drawdown:.4f}\")\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "\n",
        "# === Export result ===\n",
        "df.to_csv(\"btc_strategy_pnl_output.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPq1lSliD17N"
      },
      "source": [
        "# Backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ExCQszDT3Av"
      },
      "outputs": [],
      "source": [
        "class Strategy(ABC):\n",
        "    @abstractmethod\n",
        "    def generate_signals(self, data: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"\n",
        "        data: OHLCV (and extra) up to tâ€‘1\n",
        "        returns: Series indexed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    like data of +1 (long), -1 (short), 0 (flat)\n",
        "        \"\"\"\n",
        "\n",
        "class RegimeBasedStrategy(Strategy):\n",
        "    def __init__(self, regime_column: str, long_regimes: list, short_regimes: list):\n",
        "        self.regime_column = regime_column\n",
        "        self.long_regimes = long_regimes\n",
        "        self.short_regimes = short_regimes\n",
        "\n",
        "    def generate_signals(self, data: pd.DataFrame) -> pd.Series:\n",
        "        signals = pd.Series(index=data.index, dtype=int)\n",
        "        signals[:] = 0  # Flat by default\n",
        "\n",
        "        signals[data[self.regime_column].isin(self.long_regimes)] = 1\n",
        "        signals[data[self.regime_column].isin(self.short_regimes)] = -1\n",
        "\n",
        "        return signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nnxH_Y7mN3w"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "import pandas as pd\n",
        "\n",
        "class EasyPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        strategy: Any,\n",
        "        trade_fee: float = 0.0005\n",
        "    ):\n",
        "        # Validate data type\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            raise ValueError(\"`data` must be a pandas DataFrame.\")\n",
        "        # Ensure a 'close' column exists (or rename 'close_price')\n",
        "        if 'close' not in data.columns:\n",
        "            if 'close_price' in data.columns:\n",
        "                data = data.rename(columns={'close_price': 'close'})\n",
        "            else:\n",
        "                raise ValueError(\"`data` must contain a 'close' or 'close_price' column.\")\n",
        "        self.data = data.copy()\n",
        "        self.strategy = strategy\n",
        "        self.trade_fee = trade_fee\n",
        "\n",
        "        # Pre-generate and validate signals as positions\n",
        "        positions = self.strategy.generate_signals(self.data)\n",
        "        if not isinstance(positions, pd.Series):\n",
        "            raise ValueError(\"Signals must be returned as a pandas Series.\")\n",
        "        invalid = set(positions.unique()) - {-1, 0, 1}\n",
        "        if invalid:\n",
        "            raise ValueError(f\"Signals may only contain -1, 0, or 1; found {invalid}.\")\n",
        "        # Align index\n",
        "        positions = positions.reindex(self.data.index).fillna(0).astype(int)\n",
        "        self.positions = positions\n",
        "\n",
        "    def run_pipeline(self) -> pd.DataFrame:\n",
        "        # Run the backtest using pre-validated data & positions\n",
        "        bt = Backtester(self.data, self.positions, trade_fee=self.trade_fee)\n",
        "        return bt.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdXDIGtT3Sgh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from strategies.regime_based_strategy import RegimeBasedStrategy\n",
        "from backtest.backtester import Backtester\n",
        "\n",
        "# Load model and scaler\n",
        "model = joblib.load(\"models/Enhanced_HMM_Model.pkl\")\n",
        "scaler = joblib.load(\"models/Feature_Scaler.pkl\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"btc_features_output.csv\")\n",
        "df.rename(columns={'unified_close': 'price'}, inplace=True)\n",
        "df['start_time'] = pd.to_datetime(df['start_time'], unit='ms')\n",
        "df = df.sort_values(\"start_time\").reset_index(drop=True)\n",
        "\n",
        "# Define features\n",
        "features = [\n",
        "    \"gn_distribution_balance_wbtc\",\n",
        "    \"gn_addresses_supply_balance_more_100k\",\n",
        "    \"gn_derivatives_futures_open_interest_crypto_margin_sum\",\n",
        "    \"gn_supply_active_3y_5y\",\n",
        "    \"gn_market_realized_volatility_3_months\",\n",
        "    \"cq_reserve_usd\",\n",
        "    \"gn_supply_active_more_1y_percent\",\n",
        "    \"gn_supply_illiquid_sum\",\n",
        "    \"gn_blockchain_utxo_loss_count\",\n",
        "    \"gn_derivatives_futures_annualized_basis_3m\"\n",
        "]  # Same as before\n",
        "\n",
        "# Feature engineering\n",
        "df['price_1d_change'] = df['price'].pct_change(1)\n",
        "df['price_7d_change'] = df['price'].pct_change(7)\n",
        "df['price_volatility'] = df['price'].pct_change().rolling(7).std().fillna(0)\n",
        "\n",
        "# Scaling features\n",
        "X_test = df[features].fillna(df[features].mean())\n",
        "X_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Predict regimes\n",
        "df['predicted_regime'] = model.predict(X_scaled)\n",
        "\n",
        "# Initialize strategy\n",
        "strategy = RegimeBasedStrategy(\"predicted_regime\", [0, 3], [1, 2])\n",
        "df['signal'] = strategy.generate_signals(df)\n",
        "\n",
        "# Backtest\n",
        "backtester = Backtester(df['price'], df['signal'])\n",
        "results = backtester.run()\n",
        "\n",
        "# Plot performance\n",
        "backtester.plot_performance(results, df['start_time'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "WIF3009",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
